{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "import numpy as np\n",
    "import models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_file = 'D:/zhuomian/file/弱网中鲁棒性的恶意节点检测/exp/dataset/20241106_testTX_constantUniform_simtime-100_num_nodes-50_BHradio-0_SFradio-0_fieldLength-20_uniform-1/ProcessedMonitorSnifferRx.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
      " list([13, 14, 15, 16, 17]) list([18, 19, 20, 21, 22, 23, 24])\n",
      " list([25, 26, 27, 28, 29, 30, 31, 32, 33])\n",
      " list([34, 35, 36, 37, 38, 39, 40, 41, 42])\n",
      " list([43, 44, 45, 46, 47, 48, 49, 50]) list([51, 52, 53, 54, 55, 56, 57])\n",
      " list([58, 59, 60, 61, 62, 63]) list([64, 65, 66, 67, 68, 69, 70, 71])\n",
      " list([72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82])\n",
      " list([83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93])\n",
      " list([94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104])\n",
      " list([105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116])\n",
      " list([117, 118, 119, 120, 121, 122, 123, 124, 125, 126])\n",
      " list([127, 128, 129, 130, 131, 132, 133, 134])\n",
      " list([135, 136, 137, 138, 139, 140, 141, 142, 143])\n",
      " list([144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155])\n",
      " list([156, 157, 158, 159, 160, 161, 162, 163, 164, 165])\n",
      " list([166, 167, 168, 169, 170, 171, 172, 173, 174, 175])\n",
      " list([176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188])\n",
      " list([189, 190, 191, 192, 193, 194, 195, 196])\n",
      " list([197, 198, 199, 200, 201, 202, 203, 204])\n",
      " list([205, 206, 207, 208, 209, 210, 211, 212])\n",
      " list([213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225])\n",
      " list([226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238])\n",
      " list([239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249])\n",
      " list([250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263])\n",
      " list([264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275])\n",
      " list([276, 277, 278, 279, 280, 281, 282, 283, 284])\n",
      " list([285, 286, 287, 288, 289, 290, 291, 292])\n",
      " list([293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304])\n",
      " list([305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317])\n",
      " list([318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330])\n",
      " list([331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344])\n",
      " list([345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355])\n",
      " list([356, 357, 358, 359, 360, 361, 362, 363, 364])\n",
      " list([365, 366, 367, 368, 369, 370, 371])\n",
      " list([372, 373, 374, 375, 376, 377, 378, 379, 380, 381])\n",
      " list([382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392])\n",
      " list([393, 394, 395, 396, 397, 398, 399, 400, 401])\n",
      " list([402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413])\n",
      " list([414, 415, 416, 417, 418, 419, 420, 421, 422, 423])\n",
      " list([424, 425, 426, 427, 428, 429, 430])\n",
      " list([431, 432, 433, 434, 435, 436])\n",
      " list([437, 438, 439, 440, 441, 442, 443])\n",
      " list([444, 445, 446, 447, 448, 449, 450, 451, 452])\n",
      " list([453, 454, 455, 456, 457, 458, 459, 460, 461])\n",
      " list([462, 463, 464, 465, 466, 467, 468, 469, 470])\n",
      " list([471, 472, 473, 474, 475, 476, 477])\n",
      " list([478, 479, 480, 481, 482, 483])]\n",
      "[478 479 480 481 482 483]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[13 14 15 16 17]\n",
      "[18 19 20 21 22 23 24]\n",
      "[25 26 27 28 29 30 31 32 33]\n",
      "[34 35 36 37 38 39 40 41 42]\n",
      "[43 44 45 46 47 48 49 50]\n",
      "[51 52 53 54 55 56 57]\n",
      "[58 59 60 61 62 63]\n",
      "[64 65 66 67 68 69 70 71]\n",
      "[72 73 74 75 76 77 78 79 80 81 82]\n",
      "[83 84 85 86 87 88 89 90 91 92 93]\n",
      "[ 94  95  96  97  98  99 100 101 102 103 104]\n",
      "[105 106 107 108 109 110 111 112 113 114 115 116]\n",
      "[117 118 119 120 121 122 123 124 125 126]\n",
      "[127 128 129 130 131 132 133 134]\n",
      "[135 136 137 138 139 140 141 142 143]\n",
      "[144 145 146 147 148 149 150 151 152 153 154 155]\n",
      "[156 157 158 159 160 161 162 163 164 165]\n",
      "[166 167 168 169 170 171 172 173 174 175]\n",
      "[176 177 178 179 180 181 182 183 184 185 186 187 188]\n",
      "[189 190 191 192 193 194 195 196]\n",
      "[197 198 199 200 201 202 203 204]\n",
      "[205 206 207 208 209 210 211 212]\n",
      "[213 214 215 216 217 218 219 220 221 222 223 224 225]\n",
      "[226 227 228 229 230 231 232 233 234 235 236 237 238]\n",
      "[239 240 241 242 243 244 245 246 247 248 249]\n",
      "[250 251 252 253 254 255 256 257 258 259 260 261 262 263]\n",
      "[264 265 266 267 268 269 270 271 272 273 274 275]\n",
      "[276 277 278 279 280 281 282 283 284]\n",
      "[285 286 287 288 289 290 291 292]\n",
      "[293 294 295 296 297 298 299 300 301 302 303 304]\n",
      "[305 306 307 308 309 310 311 312 313 314 315 316 317]\n",
      "[318 319 320 321 322 323 324 325 326 327 328 329 330]\n",
      "[331 332 333 334 335 336 337 338 339 340 341 342 343 344]\n",
      "[345 346 347 348 349 350 351 352 353 354 355]\n",
      "[356 357 358 359 360 361 362 363 364]\n",
      "[365 366 367 368 369 370 371]\n",
      "[372 373 374 375 376 377 378 379 380 381]\n",
      "[382 383 384 385 386 387 388 389 390 391 392]\n",
      "[393 394 395 396 397 398 399 400 401]\n",
      "[402 403 404 405 406 407 408 409 410 411 412 413]\n",
      "[414 415 416 417 418 419 420 421 422 423]\n",
      "[424 425 426 427 428 429 430]\n",
      "[431 432 433 434 435 436]\n",
      "[437 438 439 440 441 442 443]\n",
      "[444 445 446 447 448 449 450 451 452]\n",
      "[453 454 455 456 457 458 459 460 461]\n",
      "[462 463 464 465 466 467 468 469 470]\n",
      "[471 472 473 474 475 476 477]\n",
      "[list([0, 1, 2, 3, 4, 5, 6, 7]) list([8, 9, 10, 11, 12, 13])\n",
      " list([14, 15, 16, 17, 18, 19, 20, 21])\n",
      " list([22, 23, 24, 25, 26, 27, 28, 29, 30])\n",
      " list([31, 32, 33, 34, 35, 36, 37, 38]) list([39, 40, 41, 42, 43, 44])\n",
      " list([45, 46, 47, 48, 49, 50, 51]) list([52, 53, 54, 55])\n",
      " list([56, 57, 58, 59, 60, 61, 62])\n",
      " list([63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73])\n",
      " list([74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84])\n",
      " list([85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96])\n",
      " list([97, 98, 99, 100, 101, 102, 103])\n",
      " list([104, 105, 106, 107, 108, 109, 110, 111, 112, 113])\n",
      " list([114, 115, 116, 117, 118, 119, 120])\n",
      " list([121, 122, 123, 124, 125, 126, 127])\n",
      " list([128, 129, 130, 131, 132, 133, 134, 135, 136, 137])\n",
      " list([138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150])\n",
      " list([151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162])\n",
      " list([163, 164, 165, 166, 167, 168, 169, 170, 171])\n",
      " list([172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182])\n",
      " list([183, 184, 185, 186, 187, 188, 189, 190])\n",
      " list([191, 192, 193, 194, 195, 196, 197])\n",
      " list([198, 199, 200, 201, 202, 203, 204, 205])\n",
      " list([206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216])\n",
      " list([217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230])\n",
      " list([231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243])\n",
      " list([244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256])\n",
      " list([257, 258, 259, 260, 261, 262, 263, 264])\n",
      " list([265, 266, 267, 268, 269, 270])\n",
      " list([271, 272, 273, 274, 275, 276, 277, 278])\n",
      " list([279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289])\n",
      " list([290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303])\n",
      " list([304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316])\n",
      " list([317, 318, 319, 320, 321, 322, 323, 324, 325, 326])\n",
      " list([327, 328, 329, 330, 331, 332]) list([333, 334, 335, 336, 337])\n",
      " list([338, 339, 340, 341, 342, 343, 344, 345])\n",
      " list([346, 347, 348, 349, 350, 351, 352, 353, 354])\n",
      " list([355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365])\n",
      " list([366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377])\n",
      " list([378, 379, 380, 381, 382, 383, 384, 385, 386, 387])\n",
      " list([388, 389, 390, 391, 392, 393, 394]) list([395, 396, 397, 398, 399])\n",
      " list([400, 401, 402, 403, 404, 405]) list([406, 407, 408, 409, 410])\n",
      " list([411, 412, 413, 414, 415, 416, 417, 418])\n",
      " list([419, 420, 421, 422, 423, 424, 425, 426])\n",
      " list([427, 428, 429, 430, 431, 432, 433]) list([434, 435, 436, 437, 438])]\n",
      "[434 435 436 437 438]\n",
      "[0 1 2 3 4 5 6 7]\n",
      "[ 8  9 10 11 12 13]\n",
      "[14 15 16 17 18 19 20 21]\n",
      "[22 23 24 25 26 27 28 29 30]\n",
      "[31 32 33 34 35 36 37 38]\n",
      "[39 40 41 42 43 44]\n",
      "[45 46 47 48 49 50 51]\n",
      "[52 53 54 55]\n",
      "[56 57 58 59 60 61 62]\n",
      "[63 64 65 66 67 68 69 70 71 72 73]\n",
      "[74 75 76 77 78 79 80 81 82 83 84]\n",
      "[85 86 87 88 89 90 91 92 93 94 95 96]\n",
      "[ 97  98  99 100 101 102 103]\n",
      "[104 105 106 107 108 109 110 111 112 113]\n",
      "[114 115 116 117 118 119 120]\n",
      "[121 122 123 124 125 126 127]\n",
      "[128 129 130 131 132 133 134 135 136 137]\n",
      "[138 139 140 141 142 143 144 145 146 147 148 149 150]\n",
      "[151 152 153 154 155 156 157 158 159 160 161 162]\n",
      "[163 164 165 166 167 168 169 170 171]\n",
      "[172 173 174 175 176 177 178 179 180 181 182]\n",
      "[183 184 185 186 187 188 189 190]\n",
      "[191 192 193 194 195 196 197]\n",
      "[198 199 200 201 202 203 204 205]\n",
      "[206 207 208 209 210 211 212 213 214 215 216]\n",
      "[217 218 219 220 221 222 223 224 225 226 227 228 229 230]\n",
      "[231 232 233 234 235 236 237 238 239 240 241 242 243]\n",
      "[244 245 246 247 248 249 250 251 252 253 254 255 256]\n",
      "[257 258 259 260 261 262 263 264]\n",
      "[265 266 267 268 269 270]\n",
      "[271 272 273 274 275 276 277 278]\n",
      "[279 280 281 282 283 284 285 286 287 288 289]\n",
      "[290 291 292 293 294 295 296 297 298 299 300 301 302 303]\n",
      "[304 305 306 307 308 309 310 311 312 313 314 315 316]\n",
      "[317 318 319 320 321 322 323 324 325 326]\n",
      "[327 328 329 330 331 332]\n",
      "[333 334 335 336 337]\n",
      "[338 339 340 341 342 343 344 345]\n",
      "[346 347 348 349 350 351 352 353 354]\n",
      "[355 356 357 358 359 360 361 362 363 364 365]\n",
      "[366 367 368 369 370 371 372 373 374 375 376 377]\n",
      "[378 379 380 381 382 383 384 385 386 387]\n",
      "[388 389 390 391 392 393 394]\n",
      "[395 396 397 398 399]\n",
      "[400 401 402 403 404 405]\n",
      "[406 407 408 409 410]\n",
      "[411 412 413 414 415 416 417 418]\n",
      "[419 420 421 422 423 424 425 426]\n",
      "[427 428 429 430 431 432 433]\n",
      "[list([0, 1, 2, 3, 4]) list([5, 6, 7, 8, 9, 10])\n",
      " list([11, 12, 13, 14, 15, 16]) list([17, 18, 19, 20, 21])\n",
      " list([22, 23, 24, 25, 26]) list([27, 28, 29]) list([30])\n",
      " list([31, 32, 33]) list([34, 35, 36, 37]) list([38, 39, 40, 41, 42, 43])\n",
      " list([44, 45, 46, 47, 48]) list([49, 50, 51, 52]) list([53, 54, 55, 56])\n",
      " list([57, 58]) list([59, 60]) list([61, 62]) list([63, 64])\n",
      " list([65, 66]) list([67]) list([68]) list([69]) list([70]) list([71])\n",
      " list([72]) list([73]) list([74]) list([75]) list([76])]\n",
      "[76]\n",
      "[0 1 2 3 4]\n",
      "[ 5  6  7  8  9 10]\n",
      "[11 12 13 14 15 16]\n",
      "[17 18 19 20 21]\n",
      "[22 23 24 25 26]\n",
      "[27 28 29]\n",
      "[30]\n",
      "[31 32 33]\n",
      "[34 35 36 37]\n",
      "[38 39 40 41 42 43]\n",
      "[44 45 46 47 48]\n",
      "[49 50 51 52]\n",
      "[53 54 55 56]\n",
      "[57 58]\n",
      "[59 60]\n",
      "[61 62]\n",
      "[63 64]\n",
      "[65 66]\n",
      "[67]\n",
      "[68]\n",
      "[69]\n",
      "[70]\n",
      "[71]\n",
      "[72]\n",
      "[73]\n",
      "[74]\n",
      "[75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\zhuomian\\file\\弱网中鲁棒性的恶意节点检测\\exp\\utils.py:389: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:653.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    }
   ],
   "source": [
    "# np.set_printoptions(threshold=np.inf)  # 这将打印所有内容，去掉折叠\n",
    "processed_data, adj, T, group_num = load_data(1,data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 设置numpy的打印选项，显示更多行列\n",
    "\n",
    "\n",
    "# 打印节点特征（processed_data.x）\n",
    "# print(T)\n",
    "# print(adj[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "# 打印边连接关系（processed_data.edge_index）\n",
    "print(processed_data[-1].edge_index.numpy().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616\n"
     ]
    }
   ],
   "source": [
    "# 打印节点标签（processed_data.y）\n",
    "# print(processed_data[0].y.numpy())\n",
    "print(processed_data[-1].x[:,0:8].numpy().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HypergraphModel(\n",
      "  (layer1): HypergraphConvLayer(\n",
      "    (conv): HypergraphConv(8, 64)\n",
      "  )\n",
      "  (layer2): HypergraphConvLayer(\n",
      "    (conv): HypergraphConv(64, 32)\n",
      "  )\n",
      "  (edge_conv): GraphConvolution()\n",
      "  (class_classifier): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      "  (norm1): CustomBatchNorm()\n",
      "  (norm2): CustomBatchNorm()\n",
      "  (norm3): CustomBatchNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_model = models.HypergraphModel(8, 3)\n",
    "print(train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from utils import accuracy\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "criteria = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 5e-4\n",
    "optimizer = optim.Adam(train_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "acc_measure = accuracy\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    train_model.train()\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    # print(processed_data[0].x[0:8].shape)\n",
    "    # print(processed_data[0].edge_weights.shape)\n",
    "    # print(processed_data[0].edge_index.shape)\n",
    "    for i in range(group_num):\n",
    "        print(i)\n",
    "        output = train_model(processed_data[1].x[:, 0:8], processed_data[1].edge_index, \n",
    "                                 processed_data[1].edge_weights, processed_data[1].x[:, 8:11], adj[1], T[1])\n",
    "        # print(output)\n",
    "        labels = processed_data[1].y\n",
    "        loss_train = criteria(output, labels)\n",
    "\n",
    "        acc_train = acc_measure(output, labels)\n",
    "        \n",
    "        loss_train.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_model.eval()\n",
    "        output = train_model(processed_data[1].x[:, 0:8], processed_data[1].edge_index, \n",
    "                                 processed_data[1].edge_weights, processed_data[1].x[:, 8:11], adj[1], T[1])\n",
    "        # print(output)\n",
    "        loss_val = criteria(output, labels)\n",
    "        acc_val = acc_measure(output, labels)\n",
    "        print(\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    train_model.eval()\n",
    "    output = train_model(processed_data[1].x[:, 0:8], processed_data[1].edge_index, \n",
    "                             processed_data[1].edge_weights, processed_data[1].x[:, 8:11], adj[1], T[1])\n",
    "    labels = processed_data[0].y\n",
    "    loss_test = criteria(output, labels)\n",
    "    acc_test = acc_measure(labels, output)\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    return acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loss_train: 0.7840 acc_train: 0.1000 loss_val: 0.7486 acc_val: 0.9000 time: 0.0195s\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 32]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m val_watch \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m----> 6\u001B[0m     val_watch\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      7\u001B[0m     test()\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m>\u001B[39m early_stopping \u001B[38;5;129;01mand\u001B[39;00m val_watch[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(val_watch[\u001B[38;5;241m-\u001B[39m(early_stopping \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]):\n",
      "Cell \u001B[1;32mIn[8], line 30\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(epoch)\u001B[0m\n\u001B[0;32m     26\u001B[0m loss_train \u001B[38;5;241m=\u001B[39m criteria(output, labels)\n\u001B[0;32m     28\u001B[0m acc_train \u001B[38;5;241m=\u001B[39m acc_measure(output, labels)\n\u001B[1;32m---> 30\u001B[0m \u001B[43mloss_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     33\u001B[0m train_model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\hypergraph\\lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\hypergraph\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\hypergraph\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 32]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "num_epochs = 100\n",
    "early_stopping = 30\n",
    "val_watch = []\n",
    "for epoch in range(num_epochs):\n",
    "    val_watch.append(train(epoch))\n",
    "    test()\n",
    "    if epoch > early_stopping and val_watch[-1] > np.mean(val_watch[-(early_stopping + 1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "print(\"Printing the weights : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}