{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_ns3_csv\n",
    "input_filenames =  [\n",
    "    '20241119_st-100_N-50_BH-5_SF-0_L-15_uni-1',\n",
    "    '20241119_st-100_N-50_BH-10_SF-0_L-15_uni-1',\n",
    "    '20241119_st-100_N-50_BH-15_SF-0_L-15_uni-1',\n",
    "    '20241119_st-100_N-50_BH-20_SF-0_L-15_uni-1',\n",
    "    '20241119_st-100_N-100_BH-10_SF-0_L-20_uni-1',\n",
    "    '20241119_st-100_N-100_BH-20_SF-0_L-20_uni-1',\n",
    "    '20241119_st-100_N-100_BH-30_SF-0_L-20_uni-1',\n",
    "    '20241119_st-100_N-100_BH-40_SF-0_L-20_uni-1',\n",
    "    # 其他的 filenames...\n",
    "]\n",
    "\n",
    "input_log_files = [\n",
    "    '/log1119_BH0.1_uniform_1515seed_12345.txt',\n",
    "    '/log1119_BH0.2_uniform_1515seed_12345.txt',\n",
    "    '/log1119_BH0.3_uniform_1515seed_12345.txt',\n",
    "    '/log1119_BH0.4_uniform_1515seed_12345.txt',\n",
    "    '/log1119_BH0.1_uniform_2020seed_12345.txt',\n",
    "    '/log1119_BH0.2_uniform_2020seed_12345.txt',\n",
    "    '/log1119_BH0.3_uniform_2020seed_12345.txt',\n",
    "    '/log1119_BH0.4_uniform_2020seed_12345.txt',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(input_filenames)):\n",
    "    filename = input_filenames[i]\n",
    "    log_file = './dataset/' + filename + input_log_files[i]\n",
    "    input_file = './dataset/' + filename + '/MonitorSnifferRx.csv'\n",
    "    output_file = './dataset/' + filename + '/ProcessedMonitorSnifferRx_Time10.csv'\n",
    "    # process_ns3_csv(input_file, output_file, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "import numpy as np\n",
    "import models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangchenlong/WWSNhypergraph/utils.py:519: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    }
   ],
   "source": [
    "#训练集1\n",
    "# np.set_printoptions(threshold=np.inf)  # 这将打印所有内容，去掉折叠\n",
    "data_file ='./dataset/' + '20241119_st-100_N-100_BH-40_SF-0_L-20_uni-1' + '/ProcessedMonitorSnifferRx_Time10.csv'\n",
    "train_processed_data, train_adj, train_T, train_group_num = load_data(1,data_file)\n",
    "# print(train_processed_data[0].edge_attr)\n",
    "edge_feature_size = train_processed_data[0].edge_attr.shape[1]\n",
    "nodes_feature_size = train_processed_data[0].x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练集2\n",
    "# train_processed_data = []\n",
    "# train_adj = []\n",
    "# train_T = []\n",
    "# for i in range(len(input_filenames)):\n",
    "#     filename = input_filenames[i]\n",
    "#     data_file = './dataset/' + filename + '/ProcessedMonitorSnifferRx_Time10.csv'\n",
    "#     processed_data, adj, T, _ = load_data(1,data_file)\n",
    "#     train_processed_data.append(processed_data[0])\n",
    "#     train_adj.append(adj[0])\n",
    "#     train_T.append(T[0])\n",
    "# train_group_num = len(input_filenames)\n",
    "# nodes_feature_size = train_processed_data[0].x.shape[1]\n",
    "# edge_feature_size = train_processed_data[0].edge_attr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 设置numpy的打印选项，显示更多行列\n",
    "\n",
    "\n",
    "# 打印节点特征（processed_data.x.shape）\n",
    "# print(train_processed_data[0].x)\n",
    "# print(adj[12])\n",
    "# print(T[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印边连接关系（processed_data.edge_index）\n",
    "# print(processed_data[-1].edge_index.numpy().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 打印节点标签（processed_data.y）\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "# print(processed_data[0].y.numpy())\n",
    "# print(processed_data[0].x[:, 0:23].numpy())\n",
    "# print(processed_data[5].y.numpy())\n",
    "# print(processed_data[5].edge_index)\n",
    "# print(processed_data[5].edge_weights)\n",
    "# print(processed_data[5].x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HypergraphModel(\n",
      "  (layer1): HypergraphConvLayer(\n",
      "    (conv): HypergraphConv(3, 32)\n",
      "  )\n",
      "  (edge_conv): GraphConvolution()\n",
      "  (class_classifier): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      "  (norm1): CustomBatchNorm()\n",
      "  (norm2): CustomBatchNorm()\n",
      ")\n",
      "Available GPUs: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "train_model = models.HypergraphModel(nodes_feature_size, edge_feature_size)\n",
    "print(train_model)\n",
    "# 查看可用的 GPU 数量\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Available GPUs: {num_gpus}\")\n",
    "\n",
    "# 打印每个 GPU 的名称\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_model = train_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from utils import accuracy, auc, precision, recall, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "criteria = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 5e-4\n",
    "optimizer = optim.Adam(train_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "acc_measure = accuracy\n",
    "train_end_index = train_group_num\n",
    "print(train_end_index)\n",
    "val_index = train_group_num - 1\n",
    "def train(epoch, save_path):\n",
    "    t = time.time()\n",
    "    train_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # print(processed_data[0].x[0:23].shape)\n",
    "    # print(processed_data[0].edge_weights.shape)\n",
    "    # print(processed_data[0].edge_index.shape)\n",
    "    for i in range(train_end_index):\n",
    "        # print(i)\n",
    "        output = train_model(train_processed_data[i].x.to(device),\n",
    "                             train_processed_data[i].edge_index.to(device), \n",
    "                             train_processed_data[i].edge_weights.to(device), \n",
    "                             train_processed_data[i].edge_attr.to(device), \n",
    "                             train_adj[i].to(device),\n",
    "                             train_T[i].to(device))\n",
    "        # print(output)\n",
    "        labels = train_processed_data[i].y.to(device)\n",
    "        loss_train = criteria(output, labels)\n",
    "        # print(output[:, 1])\n",
    "        acc_train = acc_measure(output, labels)\n",
    "        \n",
    "        loss_train.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        train_model.eval()\n",
    "        output = train_model(train_processed_data[val_index].x.to(device),\n",
    "                             train_processed_data[val_index].edge_index.to(device), \n",
    "                             train_processed_data[val_index].edge_weights.to(device),\n",
    "                             train_processed_data[val_index].edge_attr.to(device),\n",
    "                             train_adj[val_index].to(device),\n",
    "                             train_T[val_index].to(device))\n",
    "        # print(output)\n",
    "        labels = train_processed_data[val_index].y.to(device)\n",
    "        loss_val = criteria(output, labels)\n",
    "        acc_val = acc_measure(output, labels)\n",
    "        auc_val = auc(output, labels)\n",
    "        per_val = precision(output, labels)\n",
    "        recall_val = recall(output, labels)\n",
    "        f1_val = f1_score(output, labels)\n",
    "        # print(\n",
    "        #     'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "        #     'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "        #     'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "        #     'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "        #     'auc_val: {:.4f}'.format(auc_val),\n",
    "        #     'per_val: {:.4f}'.format(per_val),\n",
    "        #     'recall_val: {:.4f}'.format(recall_val),\n",
    "        #     'f1_val: {:.4f}'.format(f1_val),\n",
    "        #     'time: {:.4f}s'.format(time.time() - t))\n",
    "    # 每个 epoch 结束后保存模型\n",
    "    torch.save(train_model.state_dict(), save_path)\n",
    "    # torch.save({\n",
    "    #     'epoch': epoch,\n",
    "    #     'model_state_dict': train_model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     }, save_path)\n",
    "    # print(f'Model saved to {save_path}')\n",
    "\n",
    "    return loss_val.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename = '20241119_st-100_N-50_BH-5_SF-0_L-15_uni-1'\n",
    "test_data_file = './dataset/' + filename + '/ProcessedMonitorSnifferRx_Time10.csv'\n",
    "test_data, test_adj, test_T, test_group_num = load_data(1,test_data_file)\n",
    "# print(processed_data[0].edge_attr)\n",
    "end_index = len(test_data)\n",
    "print(end_index)\n",
    "top_k = 10  # 选择前10个最佳的指标值\n",
    "\n",
    "filenames = [\n",
    "    '20241119_st-100_N-50_BH-5_SF-0_L-15_uni-1',\n",
    "    '20241119_st-100_N-50_BH-10_SF-0_L-15_uni-1',\n",
    "    '20241119_st-100_N-50_BH-15_SF-0_L-15_uni-1',\n",
    "    '20241119_st-100_N-50_BH-20_SF-0_L-15_uni-1',\n",
    "    '20241119_st-100_N-100_BH-10_SF-0_L-20_uni-1',\n",
    "    '20241119_st-100_N-100_BH-20_SF-0_L-20_uni-1',\n",
    "    '20241119_st-100_N-100_BH-30_SF-0_L-20_uni-1',\n",
    "    '20241119_st-100_N-100_BH-40_SF-0_L-20_uni-1',\n",
    "    # 其他的 filenames...\n",
    "]\n",
    "num_metrics = len(filenames)\n",
    "\n",
    "dataset_results = {\n",
    "    'losses': [[[] for _ in range(end_index)] for _ in range(num_metrics)],  # 多维，记录每个数据集的多个loss值\n",
    "    'accuracies': [[[] for _ in range(end_index)] for _ in range(num_metrics)],\n",
    "    'aucs': [[[] for _ in range(end_index)] for _ in range(num_metrics)],\n",
    "    'precisions': [[[] for _ in range(end_index)] for _ in range(num_metrics)],\n",
    "    'recalls': [[[] for _ in range(end_index)] for _ in range(num_metrics)],\n",
    "    'f1_scores': [[[] for _ in range(end_index)] for _ in range(num_metrics)],\n",
    "    'TP': [[[] for _ in range(end_index)] for _ in range(num_metrics)],\n",
    "    'FP': [[[] for _ in range(end_index)] for _ in range(num_metrics)],\n",
    "    'TN': [[[] for _ in range(end_index)] for _ in range(num_metrics)],\n",
    "    'FN': [[[] for _ in range(end_index)] for _ in range(num_metrics)],\n",
    "    'outputs': [[[] for _ in range(end_index)] for _ in range(num_metrics)],\n",
    "    'labels': [[[] for _ in range(end_index)] for _ in range(num_metrics)],\n",
    "}\n",
    "\n",
    "final_results = {\n",
    "        'losses': [[] for _ in range(num_metrics)],\n",
    "        'accuracies': [[] for _ in range(num_metrics)],\n",
    "        'aucs': [[] for _ in range(num_metrics)],\n",
    "        'precisions': [[] for _ in range(num_metrics)],\n",
    "        'recalls': [[] for _ in range(num_metrics)],\n",
    "        'f1_scores': [[] for _ in range(num_metrics)],\n",
    "        'TP': [[] for _ in range(num_metrics)],\n",
    "        'FP': [[] for _ in range(num_metrics)],\n",
    "        'TN': [[] for _ in range(num_metrics)],\n",
    "        'FN': [[] for _ in range(num_metrics)],\n",
    "    }\n",
    "\n",
    "def get_top_k_avg(metric_list, top_k=top_k):\n",
    "    \"\"\"获取指标列表中的前top_k个值的均值\"\"\"\n",
    "    return np.mean(sorted(metric_list, reverse=True)[:top_k])\n",
    "\n",
    "\n",
    "        \n",
    "def test(testfilename):\n",
    "    test_data_file = './dataset/' + testfilename + '/ProcessedMonitorSnifferRx_Time10.csv'\n",
    "    test_data, test_adj, test_T, test_group_num = load_data(1,test_data_file)\n",
    "    \n",
    "    train_model.eval()\n",
    "    \n",
    "    for i in range(end_index):\n",
    "        output = train_model(test_data[i].x.to(device),\n",
    "                             test_data[i].edge_index.to(device).to(device), \n",
    "                             test_data[i].edge_weights.to(device),\n",
    "                             test_data[i].edge_attr.to(device),\n",
    "                             test_adj[i].to(device),\n",
    "                             test_T[i].to(device))\n",
    "        labels = test_data[test_group_num-1].y.to(device)\n",
    "        # print(output)\n",
    "        # print(labels)\n",
    "        loss_test = criteria(output, labels)\n",
    "        acc_test = acc_measure(output, labels)\n",
    "        auc_test = auc(output, labels)\n",
    "        per_test = precision(output, labels)\n",
    "        recall_test = recall(output, labels)\n",
    "        f1_test = f1_score(output, labels)\n",
    "        tp, tn, fp, fn = confusion_matrix(output, labels)\n",
    "\n",
    "        # 将当前指标添加到相应的数据集列表中\n",
    "        dataset_results['losses'][filenames.index(testfilename)][i].append(loss_test.item())\n",
    "        dataset_results['accuracies'][filenames.index(testfilename)][i].append(acc_test.item())\n",
    "        dataset_results['aucs'][filenames.index(testfilename)][i].append(auc_test)\n",
    "        dataset_results['precisions'][filenames.index(testfilename)][i].append(per_test)\n",
    "        dataset_results['recalls'][filenames.index(testfilename)][i].append(recall_test)\n",
    "        dataset_results['f1_scores'][filenames.index(testfilename)][i].append(f1_test)\n",
    "        dataset_results['TP'][filenames.index(testfilename)][i].append(tp)\n",
    "        dataset_results['FP'][filenames.index(testfilename)][i].append(fp)\n",
    "        dataset_results['TN'][filenames.index(testfilename)][i].append(tn)\n",
    "        dataset_results['FN'][filenames.index(testfilename)][i].append(fn)\n",
    "        dataset_results['outputs'][filenames.index(testfilename)][i].append(output)\n",
    "        dataset_results['labels'][filenames.index(testfilename)][i].append(labels)\n",
    "        \n",
    "        # print(\n",
    "        #     \"Dataset \" +\n",
    "        #     str(filenames.index(testfilename)) +\n",
    "        #     \" Test set \" +\n",
    "        #     str(i) +\n",
    "        #     \" results:\",\n",
    "        #     \"loss= {:.4f}\".format(loss_test.item()),\n",
    "        #     \"accuracy= {:.4f}\".format(acc_test.item()),\n",
    "        #     'auc_test: {:.4f}'.format(auc_test),\n",
    "        #     'per_test: {:.4f}'.format(per_test),\n",
    "        #     'recall_test: {:.4f}'.format(recall_test),\n",
    "        #     'f1_test: {:.4f}'.format(f1_test))\n",
    "    return acc_test.item()\n",
    "\n",
    "def get_top_k_AVEresult(testfilename):\n",
    "\n",
    "\n",
    "    # 对每个数据集（共10个数据集）计算最好的10个测试结果的均值\n",
    "    for i in range(end_index):\n",
    "        final_results['losses'][filenames.index(testfilename)].append(get_top_k_avg(dataset_results['losses'][filenames.index(testfilename)][i]))\n",
    "        final_results['accuracies'][filenames.index(testfilename)].append(get_top_k_avg(dataset_results['accuracies'][filenames.index(testfilename)][i]))\n",
    "        final_results['aucs'][filenames.index(testfilename)].append(get_top_k_avg(dataset_results['aucs'][filenames.index(testfilename)][i]))\n",
    "        final_results['precisions'][filenames.index(testfilename)].append(get_top_k_avg(dataset_results['precisions'][filenames.index(testfilename)][i]))\n",
    "        final_results['recalls'][filenames.index(testfilename)].append(get_top_k_avg(dataset_results['recalls'][filenames.index(testfilename)][i]))\n",
    "        final_results['f1_scores'][filenames.index(testfilename)].append(get_top_k_avg(dataset_results['f1_scores'][filenames.index(testfilename)][i]))\n",
    "        final_results['TP'][filenames.index(testfilename)].append(get_top_k_avg(dataset_results['TP'][filenames.index(testfilename)][i]))\n",
    "        final_results['FP'][filenames.index(testfilename)].append(get_top_k_avg(dataset_results['FP'][filenames.index(testfilename)][i]))\n",
    "        final_results['TN'][filenames.index(testfilename)].append(get_top_k_avg(dataset_results['TN'][filenames.index(testfilename)][i]))\n",
    "        final_results['FN'][filenames.index(testfilename)].append(get_top_k_avg(dataset_results['FN'][filenames.index(testfilename)][i]))\n",
    "        \n",
    "\n",
    "    # 输出每个数据集的最终结果（前10个测试结果的均值）\n",
    "    print(\"\\nFinal results (average of top 10 for each dataset):\")\n",
    "    for i in range(end_index):\n",
    "        print(f\"Dataset {i + 1}:\")\n",
    "        print(f\"  Average loss: {final_results['losses'][filenames.index(testfilename)][i]:.4f}\")\n",
    "        print(f\"  Average accuracy: {final_results['accuracies'][filenames.index(testfilename)][i]:.4f}\")\n",
    "        print(f\"  Average AUC: {final_results['aucs'][filenames.index(testfilename)][i]:.4f}\")\n",
    "        print(f\"  Average Precision: {final_results['precisions'][filenames.index(testfilename)][i]:.4f}\")\n",
    "        print(f\"  Average Recall: {final_results['recalls'][filenames.index(testfilename)][i]:.4f}\")\n",
    "        print(f\"  Average F1-score: {final_results['f1_scores'][filenames.index(testfilename)][i]:.4f}\")\n",
    "        print()\n",
    "    output_file = './dataset/' + testfilename + '/result.txt'\n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(\"Test Results:\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"\\nFinal results\" + testfilename + \"(average of top 10 for each dataset):\\n\")\n",
    "        for i in range(end_index):\n",
    "            f.write(f\"Dataset {i + 1}:\\n\")\n",
    "            f.write(f\"  Average loss: {final_results['losses'][filenames.index(testfilename)][i]:.4f}\\n\")\n",
    "            f.write(f\"  Average accuracy: {final_results['accuracies'][filenames.index(testfilename)][i]:.4f}\\n\")\n",
    "            f.write(f\"  Average AUC: {final_results['aucs'][filenames.index(testfilename)][i]:.4f}\\n\")\n",
    "            f.write(f\"  Average Precision: {final_results['precisions'][filenames.index(testfilename)][i]:.4f}\\n\")\n",
    "            f.write(f\"  Average Recall: {final_results['recalls'][filenames.index(testfilename)][i]:.4f}\\n\")\n",
    "            f.write(f\"  Average F1-score: {final_results['f1_scores'][filenames.index(testfilename)][i]:.4f}\\n\")\n",
    "            f.write(f\"  Average TP: {final_results['TP'][filenames.index(testfilename)][i]:.4f}\\n\")\n",
    "            f.write(f\"  Average FP: {final_results['FP'][filenames.index(testfilename)][i]:.4f}\\n\")\n",
    "            f.write(f\"  Average TN: {final_results['TN'][filenames.index(testfilename)][i]:.4f}\\n\")\n",
    "            f.write(f\"  Average FN: {final_results['FN'][filenames.index(testfilename)][i]:.4f}\\n\")\n",
    "            \n",
    "            f.write(\"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [01:45<00:00,  9.61s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final results (average of top 10 for each dataset):\n",
      "Dataset 1:\n",
      "  Average loss: 0.4028\n",
      "  Average accuracy: 0.8980\n",
      "  Average AUC: 0.9859\n",
      "  Average Precision: 0.5224\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.6802\n",
      "\n",
      "Dataset 2:\n",
      "  Average loss: 0.6037\n",
      "  Average accuracy: 0.9327\n",
      "  Average AUC: 0.9686\n",
      "  Average Precision: 0.6326\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.7672\n",
      "\n",
      "Dataset 3:\n",
      "  Average loss: 0.2300\n",
      "  Average accuracy: 0.9245\n",
      "  Average AUC: 1.0000\n",
      "  Average Precision: 0.6798\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.7818\n",
      "\n",
      "Dataset 4:\n",
      "  Average loss: 0.8853\n",
      "  Average accuracy: 0.7245\n",
      "  Average AUC: 0.9845\n",
      "  Average Precision: 0.2892\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.4426\n",
      "\n",
      "Dataset 5:\n",
      "  Average loss: 1.0943\n",
      "  Average accuracy: 0.6939\n",
      "  Average AUC: 0.9641\n",
      "  Average Precision: 0.2559\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.4059\n",
      "\n",
      "Dataset 6:\n",
      "  Average loss: 1.3343\n",
      "  Average accuracy: 0.5653\n",
      "  Average AUC: 0.7555\n",
      "  Average Precision: 0.1663\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.2750\n",
      "\n",
      "Dataset 7:\n",
      "  Average loss: 1.1664\n",
      "  Average accuracy: 0.6102\n",
      "  Average AUC: 0.8136\n",
      "  Average Precision: 0.1904\n",
      "  Average Recall: 0.9000\n",
      "  Average F1-score: 0.3126\n",
      "\n",
      "Dataset 8:\n",
      "  Average loss: 0.7072\n",
      "  Average accuracy: 0.7082\n",
      "  Average AUC: 0.8409\n",
      "  Average Precision: 0.2479\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.3738\n",
      "\n",
      "Dataset 9:\n",
      "  Average loss: 0.6850\n",
      "  Average accuracy: 0.7796\n",
      "  Average AUC: 0.9286\n",
      "  Average Precision: 0.3144\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.4442\n",
      "\n",
      "Dataset 10:\n",
      "  Average loss: 0.7974\n",
      "  Average accuracy: 0.8082\n",
      "  Average AUC: 0.7645\n",
      "  Average Precision: 0.3510\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.4799\n",
      "\n",
      "\n",
      "Final results (average of top 10 for each dataset):\n",
      "Dataset 1:\n",
      "  Average loss: 0.5768\n",
      "  Average accuracy: 0.9163\n",
      "  Average AUC: 0.9454\n",
      "  Average Precision: 0.7138\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.8317\n",
      "\n",
      "Dataset 2:\n",
      "  Average loss: 0.3192\n",
      "  Average accuracy: 0.9531\n",
      "  Average AUC: 0.9792\n",
      "  Average Precision: 0.8141\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.8972\n",
      "\n",
      "Dataset 3:\n",
      "  Average loss: 0.3478\n",
      "  Average accuracy: 0.9531\n",
      "  Average AUC: 0.9744\n",
      "  Average Precision: 0.8239\n",
      "  Average Recall: 0.9900\n",
      "  Average F1-score: 0.8960\n",
      "\n",
      "Dataset 4:\n",
      "  Average loss: 0.5291\n",
      "  Average accuracy: 0.9265\n",
      "  Average AUC: 0.9654\n",
      "  Average Precision: 0.7385\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.8487\n",
      "\n",
      "Dataset 5:\n",
      "  Average loss: 0.6432\n",
      "  Average accuracy: 0.8755\n",
      "  Average AUC: 0.9126\n",
      "  Average Precision: 0.6615\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.7241\n",
      "\n",
      "Dataset 6:\n",
      "  Average loss: 0.4960\n",
      "  Average accuracy: 0.8939\n",
      "  Average AUC: 0.8564\n",
      "  Average Precision: 0.7152\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.7550\n",
      "\n",
      "Dataset 7:\n",
      "  Average loss: 0.7829\n",
      "  Average accuracy: 0.8653\n",
      "  Average AUC: 0.8336\n",
      "  Average Precision: 0.6381\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.7091\n",
      "\n",
      "Dataset 8:\n",
      "  Average loss: 0.8899\n",
      "  Average accuracy: 0.8020\n",
      "  Average AUC: 0.8218\n",
      "  Average Precision: 0.5172\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.6262\n",
      "\n",
      "Dataset 9:\n",
      "  Average loss: 1.2608\n",
      "  Average accuracy: 0.7204\n",
      "  Average AUC: 0.6659\n",
      "  Average Precision: 0.4192\n",
      "  Average Recall: 0.5900\n",
      "  Average F1-score: 0.4739\n",
      "\n",
      "Dataset 10:\n",
      "  Average loss: 1.0480\n",
      "  Average accuracy: 0.7796\n",
      "  Average AUC: 0.8110\n",
      "  Average Precision: 0.4756\n",
      "  Average Recall: 0.7000\n",
      "  Average F1-score: 0.5610\n",
      "\n",
      "\n",
      "Final results (average of top 10 for each dataset):\n",
      "Dataset 1:\n",
      "  Average loss: 0.2473\n",
      "  Average accuracy: 0.9143\n",
      "  Average AUC: 0.9300\n",
      "  Average Precision: 0.8646\n",
      "  Average Recall: 0.8667\n",
      "  Average F1-score: 0.8632\n",
      "\n",
      "Dataset 2:\n",
      "  Average loss: 0.2601\n",
      "  Average accuracy: 0.9061\n",
      "  Average AUC: 0.9308\n",
      "  Average Precision: 0.8610\n",
      "  Average Recall: 0.8267\n",
      "  Average F1-score: 0.8432\n",
      "\n",
      "Dataset 3:\n",
      "  Average loss: 0.3340\n",
      "  Average accuracy: 0.8980\n",
      "  Average AUC: 0.8567\n",
      "  Average Precision: 0.9538\n",
      "  Average Recall: 0.7133\n",
      "  Average F1-score: 0.8096\n",
      "\n",
      "Dataset 4:\n",
      "  Average loss: 0.4524\n",
      "  Average accuracy: 0.8612\n",
      "  Average AUC: 0.7751\n",
      "  Average Precision: 0.9329\n",
      "  Average Recall: 0.6000\n",
      "  Average F1-score: 0.7277\n",
      "\n",
      "Dataset 5:\n",
      "  Average loss: 0.4995\n",
      "  Average accuracy: 0.8653\n",
      "  Average AUC: 0.8776\n",
      "  Average Precision: 0.8123\n",
      "  Average Recall: 0.7333\n",
      "  Average F1-score: 0.7700\n",
      "\n",
      "Dataset 6:\n",
      "  Average loss: 0.5956\n",
      "  Average accuracy: 0.8061\n",
      "  Average AUC: 0.8120\n",
      "  Average Precision: 0.7071\n",
      "  Average Recall: 0.6867\n",
      "  Average F1-score: 0.6827\n",
      "\n",
      "Dataset 7:\n",
      "  Average loss: 0.7940\n",
      "  Average accuracy: 0.7857\n",
      "  Average AUC: 0.7818\n",
      "  Average Precision: 0.6174\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.6964\n",
      "\n",
      "Dataset 8:\n",
      "  Average loss: 0.7597\n",
      "  Average accuracy: 0.8082\n",
      "  Average AUC: 0.7549\n",
      "  Average Precision: 0.7061\n",
      "  Average Recall: 0.6933\n",
      "  Average F1-score: 0.6870\n",
      "\n",
      "Dataset 9:\n",
      "  Average loss: 0.7361\n",
      "  Average accuracy: 0.7755\n",
      "  Average AUC: 0.6751\n",
      "  Average Precision: 0.7070\n",
      "  Average Recall: 0.5000\n",
      "  Average F1-score: 0.5752\n",
      "\n",
      "Dataset 10:\n",
      "  Average loss: 0.7671\n",
      "  Average accuracy: 0.7449\n",
      "  Average AUC: 0.7110\n",
      "  Average Precision: 0.6139\n",
      "  Average Recall: 0.5600\n",
      "  Average F1-score: 0.5747\n",
      "\n",
      "\n",
      "Final results (average of top 10 for each dataset):\n",
      "Dataset 1:\n",
      "  Average loss: 0.3098\n",
      "  Average accuracy: 0.9388\n",
      "  Average AUC: 0.9443\n",
      "  Average Precision: 0.9474\n",
      "  Average Recall: 0.9000\n",
      "  Average F1-score: 0.9231\n",
      "\n",
      "Dataset 2:\n",
      "  Average loss: 0.5783\n",
      "  Average accuracy: 0.8449\n",
      "  Average AUC: 0.8505\n",
      "  Average Precision: 0.8545\n",
      "  Average Recall: 0.7500\n",
      "  Average F1-score: 0.7983\n",
      "\n",
      "Dataset 3:\n",
      "  Average loss: 0.4773\n",
      "  Average accuracy: 0.8531\n",
      "  Average AUC: 0.8707\n",
      "  Average Precision: 0.8725\n",
      "  Average Recall: 0.7500\n",
      "  Average F1-score: 0.8065\n",
      "\n",
      "Dataset 4:\n",
      "  Average loss: 0.4589\n",
      "  Average accuracy: 0.8653\n",
      "  Average AUC: 0.8988\n",
      "  Average Precision: 0.8808\n",
      "  Average Recall: 0.7850\n",
      "  Average F1-score: 0.8262\n",
      "\n",
      "Dataset 5:\n",
      "  Average loss: 0.3981\n",
      "  Average accuracy: 0.8551\n",
      "  Average AUC: 0.8902\n",
      "  Average Precision: 0.8510\n",
      "  Average Recall: 0.7900\n",
      "  Average F1-score: 0.8168\n",
      "\n",
      "Dataset 6:\n",
      "  Average loss: 0.4243\n",
      "  Average accuracy: 0.8918\n",
      "  Average AUC: 0.9091\n",
      "  Average Precision: 0.8813\n",
      "  Average Recall: 0.8500\n",
      "  Average F1-score: 0.8653\n",
      "\n",
      "Dataset 7:\n",
      "  Average loss: 0.6021\n",
      "  Average accuracy: 0.8367\n",
      "  Average AUC: 0.8640\n",
      "  Average Precision: 0.8000\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.8000\n",
      "\n",
      "Dataset 8:\n",
      "  Average loss: 0.4943\n",
      "  Average accuracy: 0.8265\n",
      "  Average AUC: 0.8867\n",
      "  Average Precision: 0.8504\n",
      "  Average Recall: 0.7000\n",
      "  Average F1-score: 0.7675\n",
      "\n",
      "Dataset 9:\n",
      "  Average loss: 0.6039\n",
      "  Average accuracy: 0.8163\n",
      "  Average AUC: 0.8374\n",
      "  Average Precision: 0.9062\n",
      "  Average Recall: 0.6200\n",
      "  Average F1-score: 0.7335\n",
      "\n",
      "Dataset 10:\n",
      "  Average loss: 0.7212\n",
      "  Average accuracy: 0.7755\n",
      "  Average AUC: 0.7393\n",
      "  Average Precision: 0.8462\n",
      "  Average Recall: 0.5500\n",
      "  Average F1-score: 0.6667\n",
      "\n",
      "\n",
      "Final results (average of top 10 for each dataset):\n",
      "Dataset 1:\n",
      "  Average loss: 0.2765\n",
      "  Average accuracy: 0.9172\n",
      "  Average AUC: 0.9962\n",
      "  Average Precision: 0.5713\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.7213\n",
      "\n",
      "Dataset 2:\n",
      "  Average loss: 0.7009\n",
      "  Average accuracy: 0.7707\n",
      "  Average AUC: 0.9563\n",
      "  Average Precision: 0.3096\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.4718\n",
      "\n",
      "Dataset 3:\n",
      "  Average loss: 0.9628\n",
      "  Average accuracy: 0.6828\n",
      "  Average AUC: 0.8096\n",
      "  Average Precision: 0.2142\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.3378\n",
      "\n",
      "Dataset 4:\n",
      "  Average loss: 1.1406\n",
      "  Average accuracy: 0.6535\n",
      "  Average AUC: 0.8540\n",
      "  Average Precision: 0.2036\n",
      "  Average Recall: 0.8300\n",
      "  Average F1-score: 0.3266\n",
      "\n",
      "Dataset 5:\n",
      "  Average loss: 0.9775\n",
      "  Average accuracy: 0.7212\n",
      "  Average AUC: 0.9455\n",
      "  Average Precision: 0.2667\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.4209\n",
      "\n",
      "Dataset 6:\n",
      "  Average loss: 1.0434\n",
      "  Average accuracy: 0.6697\n",
      "  Average AUC: 0.9109\n",
      "  Average Precision: 0.2219\n",
      "  Average Recall: 0.9000\n",
      "  Average F1-score: 0.3558\n",
      "\n",
      "Dataset 7:\n",
      "  Average loss: 0.8787\n",
      "  Average accuracy: 0.7434\n",
      "  Average AUC: 0.9565\n",
      "  Average Precision: 0.2829\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.4409\n",
      "\n",
      "Dataset 8:\n",
      "  Average loss: 0.8076\n",
      "  Average accuracy: 0.7444\n",
      "  Average AUC: 0.8837\n",
      "  Average Precision: 0.2566\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.3883\n",
      "\n",
      "Dataset 9:\n",
      "  Average loss: 0.7107\n",
      "  Average accuracy: 0.7717\n",
      "  Average AUC: 0.9834\n",
      "  Average Precision: 0.3089\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.4714\n",
      "\n",
      "Dataset 10:\n",
      "  Average loss: 0.7358\n",
      "  Average accuracy: 0.7848\n",
      "  Average AUC: 0.9622\n",
      "  Average Precision: 0.3197\n",
      "  Average Recall: 1.0000\n",
      "  Average F1-score: 0.4844\n",
      "\n",
      "\n",
      "Final results (average of top 10 for each dataset):\n",
      "Dataset 1:\n",
      "  Average loss: 0.4501\n",
      "  Average accuracy: 0.9303\n",
      "  Average AUC: 0.9686\n",
      "  Average Precision: 0.7478\n",
      "  Average Recall: 0.9950\n",
      "  Average F1-score: 0.8525\n",
      "\n",
      "Dataset 2:\n",
      "  Average loss: 0.5411\n",
      "  Average accuracy: 0.8364\n",
      "  Average AUC: 0.9157\n",
      "  Average Precision: 0.5689\n",
      "  Average Recall: 0.9000\n",
      "  Average F1-score: 0.6945\n",
      "\n",
      "Dataset 3:\n",
      "  Average loss: 0.9112\n",
      "  Average accuracy: 0.6990\n",
      "  Average AUC: 0.6906\n",
      "  Average Precision: 0.3786\n",
      "  Average Recall: 0.6500\n",
      "  Average F1-score: 0.4738\n",
      "\n",
      "Dataset 4:\n",
      "  Average loss: 1.1063\n",
      "  Average accuracy: 0.6273\n",
      "  Average AUC: 0.6773\n",
      "  Average Precision: 0.2999\n",
      "  Average Recall: 0.6050\n",
      "  Average F1-score: 0.3992\n",
      "\n",
      "Dataset 5:\n",
      "  Average loss: 0.9232\n",
      "  Average accuracy: 0.7596\n",
      "  Average AUC: 0.6613\n",
      "  Average Precision: 0.4458\n",
      "  Average Recall: 0.5850\n",
      "  Average F1-score: 0.5023\n",
      "\n",
      "Dataset 6:\n",
      "  Average loss: 0.9781\n",
      "  Average accuracy: 0.7162\n",
      "  Average AUC: 0.6074\n",
      "  Average Precision: 0.3637\n",
      "  Average Recall: 0.4500\n",
      "  Average F1-score: 0.3974\n",
      "\n",
      "Dataset 7:\n",
      "  Average loss: 0.7837\n",
      "  Average accuracy: 0.7333\n",
      "  Average AUC: 0.6982\n",
      "  Average Precision: 0.3999\n",
      "  Average Recall: 0.6150\n",
      "  Average F1-score: 0.4795\n",
      "\n",
      "Dataset 8:\n",
      "  Average loss: 0.7582\n",
      "  Average accuracy: 0.7313\n",
      "  Average AUC: 0.7709\n",
      "  Average Precision: 0.4162\n",
      "  Average Recall: 0.7450\n",
      "  Average F1-score: 0.5323\n",
      "\n",
      "Dataset 9:\n",
      "  Average loss: 0.8003\n",
      "  Average accuracy: 0.7535\n",
      "  Average AUC: 0.8362\n",
      "  Average Precision: 0.4424\n",
      "  Average Recall: 0.7800\n",
      "  Average F1-score: 0.5599\n",
      "\n",
      "Dataset 10:\n",
      "  Average loss: 0.8258\n",
      "  Average accuracy: 0.7212\n",
      "  Average AUC: 0.7678\n",
      "  Average Precision: 0.3922\n",
      "  Average Recall: 0.6750\n",
      "  Average F1-score: 0.4919\n",
      "\n",
      "\n",
      "Final results (average of top 10 for each dataset):\n",
      "Dataset 1:\n",
      "  Average loss: 0.2363\n",
      "  Average accuracy: 0.9556\n",
      "  Average AUC: 0.9661\n",
      "  Average Precision: 0.9010\n",
      "  Average Recall: 0.9633\n",
      "  Average F1-score: 0.9292\n",
      "\n",
      "Dataset 2:\n",
      "  Average loss: 0.2632\n",
      "  Average accuracy: 0.9414\n",
      "  Average AUC: 0.9264\n",
      "  Average Precision: 0.9304\n",
      "  Average Recall: 0.8733\n",
      "  Average F1-score: 0.9005\n",
      "\n",
      "Dataset 3:\n",
      "  Average loss: 0.3558\n",
      "  Average accuracy: 0.9111\n",
      "  Average AUC: 0.8494\n",
      "  Average Precision: 0.9012\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.8463\n",
      "\n",
      "Dataset 4:\n",
      "  Average loss: 0.5402\n",
      "  Average accuracy: 0.8556\n",
      "  Average AUC: 0.8471\n",
      "  Average Precision: 0.7604\n",
      "  Average Recall: 0.8067\n",
      "  Average F1-score: 0.7775\n",
      "\n",
      "Dataset 5:\n",
      "  Average loss: 1.1973\n",
      "  Average accuracy: 0.7192\n",
      "  Average AUC: 0.7281\n",
      "  Average Precision: 0.5340\n",
      "  Average Recall: 0.7533\n",
      "  Average F1-score: 0.6194\n",
      "\n",
      "Dataset 6:\n",
      "  Average loss: 1.0319\n",
      "  Average accuracy: 0.6798\n",
      "  Average AUC: 0.7520\n",
      "  Average Precision: 0.4829\n",
      "  Average Recall: 0.6867\n",
      "  Average F1-score: 0.5632\n",
      "\n",
      "Dataset 7:\n",
      "  Average loss: 0.9282\n",
      "  Average accuracy: 0.7232\n",
      "  Average AUC: 0.7903\n",
      "  Average Precision: 0.5391\n",
      "  Average Recall: 0.7333\n",
      "  Average F1-score: 0.6193\n",
      "\n",
      "Dataset 8:\n",
      "  Average loss: 0.9327\n",
      "  Average accuracy: 0.7485\n",
      "  Average AUC: 0.8284\n",
      "  Average Precision: 0.5651\n",
      "  Average Recall: 0.8100\n",
      "  Average F1-score: 0.6613\n",
      "\n",
      "Dataset 9:\n",
      "  Average loss: 1.1790\n",
      "  Average accuracy: 0.7343\n",
      "  Average AUC: 0.7600\n",
      "  Average Precision: 0.5486\n",
      "  Average Recall: 0.7267\n",
      "  Average F1-score: 0.6246\n",
      "\n",
      "Dataset 10:\n",
      "  Average loss: 0.8817\n",
      "  Average accuracy: 0.7556\n",
      "  Average AUC: 0.7905\n",
      "  Average Precision: 0.5715\n",
      "  Average Recall: 0.8000\n",
      "  Average F1-score: 0.6660\n",
      "\n",
      "\n",
      "Final results (average of top 10 for each dataset):\n",
      "Dataset 1:\n",
      "  Average loss: 0.0951\n",
      "  Average accuracy: 0.9657\n",
      "  Average AUC: 0.9983\n",
      "  Average Precision: 0.9896\n",
      "  Average Recall: 0.9250\n",
      "  Average F1-score: 0.9560\n",
      "\n",
      "Dataset 2:\n",
      "  Average loss: 0.1224\n",
      "  Average accuracy: 0.9657\n",
      "  Average AUC: 0.9844\n",
      "  Average Precision: 0.9844\n",
      "  Average Recall: 0.9350\n",
      "  Average F1-score: 0.9565\n",
      "\n",
      "Dataset 3:\n",
      "  Average loss: 0.1746\n",
      "  Average accuracy: 0.9505\n",
      "  Average AUC: 0.9579\n",
      "  Average Precision: 0.9635\n",
      "  Average Recall: 0.9125\n",
      "  Average F1-score: 0.9371\n",
      "\n",
      "Dataset 4:\n",
      "  Average loss: 0.1519\n",
      "  Average accuracy: 0.9556\n",
      "  Average AUC: 0.9533\n",
      "  Average Precision: 0.9739\n",
      "  Average Recall: 0.9200\n",
      "  Average F1-score: 0.9436\n",
      "\n",
      "Dataset 5:\n",
      "  Average loss: 0.2186\n",
      "  Average accuracy: 0.9414\n",
      "  Average AUC: 0.9389\n",
      "  Average Precision: 0.9350\n",
      "  Average Recall: 0.9225\n",
      "  Average F1-score: 0.9271\n",
      "\n",
      "Dataset 6:\n",
      "  Average loss: 0.2368\n",
      "  Average accuracy: 0.9293\n",
      "  Average AUC: 0.9381\n",
      "  Average Precision: 0.9155\n",
      "  Average Recall: 0.9100\n",
      "  Average F1-score: 0.9124\n",
      "\n",
      "Dataset 7:\n",
      "  Average loss: 0.2314\n",
      "  Average accuracy: 0.9202\n",
      "  Average AUC: 0.9531\n",
      "  Average Precision: 0.9396\n",
      "  Average Recall: 0.8700\n",
      "  Average F1-score: 0.8981\n",
      "\n",
      "Dataset 8:\n",
      "  Average loss: 0.2888\n",
      "  Average accuracy: 0.9111\n",
      "  Average AUC: 0.9205\n",
      "  Average Precision: 0.9061\n",
      "  Average Recall: 0.8750\n",
      "  Average F1-score: 0.8893\n",
      "\n",
      "Dataset 9:\n",
      "  Average loss: 0.5406\n",
      "  Average accuracy: 0.8747\n",
      "  Average AUC: 0.8890\n",
      "  Average Precision: 0.8793\n",
      "  Average Recall: 0.8075\n",
      "  Average F1-score: 0.8389\n",
      "\n",
      "Dataset 10:\n",
      "  Average loss: 0.4971\n",
      "  Average accuracy: 0.8455\n",
      "  Average AUC: 0.8610\n",
      "  Average Precision: 0.9005\n",
      "  Average Recall: 0.7000\n",
      "  Average F1-score: 0.7853\n",
      "\n",
      "Optimization Finished!\n",
      "Total time elapsed: 105.6729s\n",
      "Printing the weights : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "num_epochs = 11\n",
    "early_stopping = 30\n",
    "val_watch = []\n",
    "t_total = time.time()\n",
    "current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n",
    "\n",
    "\n",
    "save_path = './models/hnff/' + current_time + '/' + current_time + '.pth'\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)  # 创建目录，如果目录已存在不抛出错误\n",
    "\n",
    "        \n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\", unit=\"epoch\"):\n",
    "    val = train(epoch, save_path)  # 训练并获得结果\n",
    "    if val is None:  # 检查是否返回了None值\n",
    "        print(f\"Warning: train(epoch) returned None for epoch {epoch}. Skipping this iteration.\")\n",
    "        continue\n",
    "    \n",
    "    val_watch.append(val)  # 将结果添加到val_watch\n",
    "    # if ((num_epochs - epoch) < 30):\n",
    "    for filename in filenames:\n",
    "        test(filename)  # 调用测试函数\n",
    "\n",
    "\n",
    "        \n",
    "for filename in filenames:\n",
    "    get_top_k_AVEresult(filename)\n",
    "\n",
    "    # if epoch > early_stopping and val_watch[-1] > np.mean(val_watch[-(early_stopping + 1):-1]):\n",
    "    #     print(\"Early stopping...\")\n",
    "    #     break\n",
    "\n",
    "\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "print(\"Printing the weights : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC curve for dataset 0 with the highest AUC (0.9955) saved to ./dataset/roc_curves_dataset0-0_highest_auc.png.\n",
      "ROC curve for dataset 0 with the highest AUC (0.9727) saved to ./dataset/roc_curves_dataset0-1_highest_auc.png.\n",
      "ROC curve for dataset 0 with the highest AUC (1.0000) saved to ./dataset/roc_curves_dataset0-2_highest_auc.png.\n",
      "ROC curve for dataset 0 with the highest AUC (0.9864) saved to ./dataset/roc_curves_dataset0-3_highest_auc.png.\n",
      "ROC curve for dataset 0 with the highest AUC (0.9682) saved to ./dataset/roc_curves_dataset0-4_highest_auc.png.\n",
      "ROC curve for dataset 0 with the highest AUC (0.7727) saved to ./dataset/roc_curves_dataset0-5_highest_auc.png.\n",
      "ROC curve for dataset 0 with the highest AUC (0.8409) saved to ./dataset/roc_curves_dataset0-6_highest_auc.png.\n",
      "ROC curve for dataset 0 with the highest AUC (0.8455) saved to ./dataset/roc_curves_dataset0-7_highest_auc.png.\n",
      "ROC curve for dataset 0 with the highest AUC (0.9500) saved to ./dataset/roc_curves_dataset0-8_highest_auc.png.\n",
      "ROC curve for dataset 0 with the highest AUC (0.7682) saved to ./dataset/roc_curves_dataset0-9_highest_auc.png.\n",
      "ROC curve for dataset 1 with the highest AUC (0.9513) saved to ./dataset/roc_curves_dataset1-0_highest_auc.png.\n",
      "ROC curve for dataset 1 with the highest AUC (0.9821) saved to ./dataset/roc_curves_dataset1-1_highest_auc.png.\n",
      "ROC curve for dataset 1 with the highest AUC (0.9744) saved to ./dataset/roc_curves_dataset1-2_highest_auc.png.\n",
      "ROC curve for dataset 1 with the highest AUC (0.9667) saved to ./dataset/roc_curves_dataset1-3_highest_auc.png.\n",
      "ROC curve for dataset 1 with the highest AUC (0.9205) saved to ./dataset/roc_curves_dataset1-4_highest_auc.png.\n",
      "ROC curve for dataset 1 with the highest AUC (0.8615) saved to ./dataset/roc_curves_dataset1-5_highest_auc.png.\n",
      "ROC curve for dataset 1 with the highest AUC (0.8359) saved to ./dataset/roc_curves_dataset1-6_highest_auc.png.\n",
      "ROC curve for dataset 1 with the highest AUC (0.8282) saved to ./dataset/roc_curves_dataset1-7_highest_auc.png.\n",
      "ROC curve for dataset 1 with the highest AUC (0.6859) saved to ./dataset/roc_curves_dataset1-8_highest_auc.png.\n",
      "ROC curve for dataset 1 with the highest AUC (0.8141) saved to ./dataset/roc_curves_dataset1-9_highest_auc.png.\n",
      "ROC curve for dataset 2 with the highest AUC (0.9333) saved to ./dataset/roc_curves_dataset2-0_highest_auc.png.\n",
      "ROC curve for dataset 2 with the highest AUC (0.9431) saved to ./dataset/roc_curves_dataset2-1_highest_auc.png.\n",
      "ROC curve for dataset 2 with the highest AUC (0.8627) saved to ./dataset/roc_curves_dataset2-2_highest_auc.png.\n",
      "ROC curve for dataset 2 with the highest AUC (0.7784) saved to ./dataset/roc_curves_dataset2-3_highest_auc.png.\n",
      "ROC curve for dataset 2 with the highest AUC (0.8843) saved to ./dataset/roc_curves_dataset2-4_highest_auc.png.\n",
      "ROC curve for dataset 2 with the highest AUC (0.8255) saved to ./dataset/roc_curves_dataset2-5_highest_auc.png.\n",
      "ROC curve for dataset 2 with the highest AUC (0.7902) saved to ./dataset/roc_curves_dataset2-6_highest_auc.png.\n",
      "ROC curve for dataset 2 with the highest AUC (0.7627) saved to ./dataset/roc_curves_dataset2-7_highest_auc.png.\n",
      "ROC curve for dataset 2 with the highest AUC (0.6804) saved to ./dataset/roc_curves_dataset2-8_highest_auc.png.\n",
      "ROC curve for dataset 2 with the highest AUC (0.7216) saved to ./dataset/roc_curves_dataset2-9_highest_auc.png.\n",
      "ROC curve for dataset 3 with the highest AUC (0.9534) saved to ./dataset/roc_curves_dataset3-0_highest_auc.png.\n",
      "ROC curve for dataset 3 with the highest AUC (0.8638) saved to ./dataset/roc_curves_dataset3-1_highest_auc.png.\n",
      "ROC curve for dataset 3 with the highest AUC (0.8759) saved to ./dataset/roc_curves_dataset3-2_highest_auc.png.\n",
      "ROC curve for dataset 3 with the highest AUC (0.9069) saved to ./dataset/roc_curves_dataset3-3_highest_auc.png.\n",
      "ROC curve for dataset 3 with the highest AUC (0.9086) saved to ./dataset/roc_curves_dataset3-4_highest_auc.png.\n",
      "ROC curve for dataset 3 with the highest AUC (0.9121) saved to ./dataset/roc_curves_dataset3-5_highest_auc.png.\n",
      "ROC curve for dataset 3 with the highest AUC (0.8672) saved to ./dataset/roc_curves_dataset3-6_highest_auc.png.\n",
      "ROC curve for dataset 3 with the highest AUC (0.8948) saved to ./dataset/roc_curves_dataset3-7_highest_auc.png.\n",
      "ROC curve for dataset 3 with the highest AUC (0.8534) saved to ./dataset/roc_curves_dataset3-8_highest_auc.png.\n",
      "ROC curve for dataset 3 with the highest AUC (0.7552) saved to ./dataset/roc_curves_dataset3-9_highest_auc.png.\n",
      "ROC curve for dataset 4 with the highest AUC (0.9978) saved to ./dataset/roc_curves_dataset4-0_highest_auc.png.\n",
      "ROC curve for dataset 4 with the highest AUC (0.9708) saved to ./dataset/roc_curves_dataset4-1_highest_auc.png.\n",
      "ROC curve for dataset 4 with the highest AUC (0.8258) saved to ./dataset/roc_curves_dataset4-2_highest_auc.png.\n",
      "ROC curve for dataset 4 with the highest AUC (0.8629) saved to ./dataset/roc_curves_dataset4-3_highest_auc.png.\n",
      "ROC curve for dataset 4 with the highest AUC (0.9573) saved to ./dataset/roc_curves_dataset4-4_highest_auc.png.\n",
      "ROC curve for dataset 4 with the highest AUC (0.9202) saved to ./dataset/roc_curves_dataset4-5_highest_auc.png.\n",
      "ROC curve for dataset 4 with the highest AUC (0.9652) saved to ./dataset/roc_curves_dataset4-6_highest_auc.png.\n",
      "ROC curve for dataset 4 with the highest AUC (0.8899) saved to ./dataset/roc_curves_dataset4-7_highest_auc.png.\n",
      "ROC curve for dataset 4 with the highest AUC (0.9865) saved to ./dataset/roc_curves_dataset4-8_highest_auc.png.\n",
      "ROC curve for dataset 4 with the highest AUC (0.9697) saved to ./dataset/roc_curves_dataset4-9_highest_auc.png.\n",
      "ROC curve for dataset 5 with the highest AUC (0.9715) saved to ./dataset/roc_curves_dataset5-0_highest_auc.png.\n",
      "ROC curve for dataset 5 with the highest AUC (0.9241) saved to ./dataset/roc_curves_dataset5-1_highest_auc.png.\n",
      "ROC curve for dataset 5 with the highest AUC (0.7032) saved to ./dataset/roc_curves_dataset5-2_highest_auc.png.\n",
      "ROC curve for dataset 5 with the highest AUC (0.7000) saved to ./dataset/roc_curves_dataset5-3_highest_auc.png.\n",
      "ROC curve for dataset 5 with the highest AUC (0.6785) saved to ./dataset/roc_curves_dataset5-4_highest_auc.png.\n",
      "ROC curve for dataset 5 with the highest AUC (0.6184) saved to ./dataset/roc_curves_dataset5-5_highest_auc.png.\n",
      "ROC curve for dataset 5 with the highest AUC (0.7089) saved to ./dataset/roc_curves_dataset5-6_highest_auc.png.\n",
      "ROC curve for dataset 5 with the highest AUC (0.7829) saved to ./dataset/roc_curves_dataset5-7_highest_auc.png.\n",
      "ROC curve for dataset 5 with the highest AUC (0.8449) saved to ./dataset/roc_curves_dataset5-8_highest_auc.png.\n",
      "ROC curve for dataset 5 with the highest AUC (0.7759) saved to ./dataset/roc_curves_dataset5-9_highest_auc.png.\n",
      "ROC curve for dataset 6 with the highest AUC (0.9700) saved to ./dataset/roc_curves_dataset6-0_highest_auc.png.\n",
      "ROC curve for dataset 6 with the highest AUC (0.9319) saved to ./dataset/roc_curves_dataset6-1_highest_auc.png.\n",
      "ROC curve for dataset 6 with the highest AUC (0.8522) saved to ./dataset/roc_curves_dataset6-2_highest_auc.png.\n",
      "ROC curve for dataset 6 with the highest AUC (0.8507) saved to ./dataset/roc_curves_dataset6-3_highest_auc.png.\n",
      "ROC curve for dataset 6 with the highest AUC (0.7449) saved to ./dataset/roc_curves_dataset6-4_highest_auc.png.\n",
      "ROC curve for dataset 6 with the highest AUC (0.7734) saved to ./dataset/roc_curves_dataset6-5_highest_auc.png.\n",
      "ROC curve for dataset 6 with the highest AUC (0.8111) saved to ./dataset/roc_curves_dataset6-6_highest_auc.png.\n",
      "ROC curve for dataset 6 with the highest AUC (0.8478) saved to ./dataset/roc_curves_dataset6-7_highest_auc.png.\n",
      "ROC curve for dataset 6 with the highest AUC (0.7768) saved to ./dataset/roc_curves_dataset6-8_highest_auc.png.\n",
      "ROC curve for dataset 6 with the highest AUC (0.8072) saved to ./dataset/roc_curves_dataset6-9_highest_auc.png.\n",
      "ROC curve for dataset 7 with the highest AUC (0.9992) saved to ./dataset/roc_curves_dataset7-0_highest_auc.png.\n",
      "ROC curve for dataset 7 with the highest AUC (0.9864) saved to ./dataset/roc_curves_dataset7-1_highest_auc.png.\n",
      "ROC curve for dataset 7 with the highest AUC (0.9623) saved to ./dataset/roc_curves_dataset7-2_highest_auc.png.\n",
      "ROC curve for dataset 7 with the highest AUC (0.9547) saved to ./dataset/roc_curves_dataset7-3_highest_auc.png.\n",
      "ROC curve for dataset 7 with the highest AUC (0.9419) saved to ./dataset/roc_curves_dataset7-4_highest_auc.png.\n",
      "ROC curve for dataset 7 with the highest AUC (0.9398) saved to ./dataset/roc_curves_dataset7-5_highest_auc.png.\n",
      "ROC curve for dataset 7 with the highest AUC (0.9581) saved to ./dataset/roc_curves_dataset7-6_highest_auc.png.\n",
      "ROC curve for dataset 7 with the highest AUC (0.9225) saved to ./dataset/roc_curves_dataset7-7_highest_auc.png.\n",
      "ROC curve for dataset 7 with the highest AUC (0.8928) saved to ./dataset/roc_curves_dataset7-8_highest_auc.png.\n",
      "ROC curve for dataset 7 with the highest AUC (0.8640) saved to ./dataset/roc_curves_dataset7-9_highest_auc.png.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc as Sauc\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_highest_auc_per_dataset(dataset_results, save_path_prefix):\n",
    "    \"\"\"\n",
    "    Plot and save the ROC curve for the highest AUC in each dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset_results (dict): The dictionary containing AUCs and corresponding outputs/labels.\n",
    "    save_path_prefix (str): The prefix for the file paths to save the ROC curve images.\n",
    "    \"\"\"\n",
    "    for dataset_idx, aucs in enumerate(dataset_results['aucs']):\n",
    "        \n",
    "\n",
    "        # Find the highest AUC in the current dataset\n",
    "        for test_idx, auc_list in enumerate(aucs):\n",
    "            highest_auc = 0\n",
    "            best_output = None\n",
    "            best_labels = None\n",
    "            for i, auc_val in enumerate(auc_list):\n",
    "                if auc_val > highest_auc:\n",
    "                    highest_auc = auc_val\n",
    "                    best_output = dataset_results['outputs'][dataset_idx][test_idx][i]\n",
    "                    best_labels = dataset_results['labels'][dataset_idx][test_idx][i]\n",
    "                    best_output = best_output\n",
    "                    best_labels = best_labels\n",
    "\n",
    "            # Plot and save the ROC curve for the highest AUC in this dataset\n",
    "            if best_output is not None and best_labels is not None:\n",
    "                save_path = f\"./dataset{save_path_prefix}_dataset{dataset_idx}-{test_idx}_highest_auc\"\n",
    "                plot_and_save_roc_curve(best_output, best_labels, save_path)\n",
    "                print(f\"ROC curve for dataset {dataset_idx} with the highest AUC ({highest_auc:.4f}) saved to {save_path}.\")\n",
    "            else:\n",
    "                print(f\"No valid data found for dataset {dataset_idx} to plot ROC curve.\")\n",
    "\n",
    "def plot_and_save_roc_curve(output, labels, save_path):\n",
    "    \"\"\"\n",
    "    Plot and save the ROC curve.\n",
    "\n",
    "    Parameters:\n",
    "    output (torch.Tensor): The model predictions (logits or probabilities).\n",
    "    labels (torch.Tensor): The ground truth labels.\n",
    "    save_path (str): The file path to save the ROC curve image.\n",
    "    \"\"\"\n",
    "    # Ensure output is in probability format (if it's logits, apply softmax/sigmoid)\n",
    "    if output.ndim > 1 and output.shape[1] > 1:  # Multi-class case (not expected for binary)\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)[:, 1].detach().cpu().numpy()\n",
    "    else:  # Binary case\n",
    "        probabilities = torch.sigmoid(output).detach().cpu().numpy()\n",
    "\n",
    "    # Ensure labels are binary (0 or 1) and one-dimensional\n",
    "    if labels.ndim > 1:\n",
    "        labels = labels.argmax(dim=1)  # Convert one-hot to class indices\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    # Compute ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(labels, probabilities)\n",
    "    roc_auc = Sauc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Save the figure as a PDF\n",
    "    plt.savefig(save_path, format='pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "plot_highest_auc_per_dataset(dataset_results, '/roc_curves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}