{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{47, 39}\n"
     ]
    }
   ],
   "source": [
    "# from utils import process_csv\n",
    "# input_file = 'D:/zhuomian/file/弱网中鲁棒性的恶意节点检测/exp/dataset/20241106_testTX_constantUniform_simtime-100_num_nodes-50_BHradio-0_SFradio-0_fieldLength-20_uniform-1/MonitorSnifferRx.csv'\n",
    "# output_file = 'D:/zhuomian/file/弱网中鲁棒性的恶意节点检测/exp/dataset/20241106_testTX_constantUniform_simtime-100_num_nodes-50_BHradio-0_SFradio-0_fieldLength-20_uniform-1/ProcessedMonitorSnifferRx.csv'\n",
    "# log_file = 'D:/zhuomian/file/弱网中鲁棒性的恶意节点检测/exp/dataset/20241106_testTX_constantUniform_simtime-100_num_nodes-50_BHradio-0_SFradio-0_fieldLength-20_uniform-1/log1106_uniform_20#%20_seed_12345.txt'\n",
    "# process_csv(input_file, output_file, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "import numpy as np\n",
    "import models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_file = 'D:/zhuomian/file/弱网中鲁棒性的恶意节点检测/exp/dataset/20241106_testTX_constantUniform_simtime-100_num_nodes-50_BHradio-0_SFradio-0_fieldLength-20_uniform-1/ProcessedMonitorSnifferRx.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=np.inf)  # 这将打印所有内容，去掉折叠\n",
    "processed_data, adj, T, group_num = load_data(1,data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 设置numpy的打印选项，显示更多行列\n",
    "\n",
    "\n",
    "# 打印节点特征（processed_data.x）\n",
    "# print(processed_data)\n",
    "# print(adj[12])\n",
    "# print(T[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印边连接关系（processed_data.edge_index）\n",
    "# print(processed_data[-1].edge_index.numpy().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 打印节点标签（processed_data.y）\n",
    "# print(processed_data[0].y.numpy())\n",
    "# print(processed_data[-1].x[:,0:8].numpy().size)\n",
    "# print(processed_data[0].y.numpy())\n",
    "# print(processed_data[12].edge_index)\n",
    "# print(processed_data[12].edge_weights)\n",
    "# print(processed_data[12].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HypergraphModel(\n",
      "  (layer1): HypergraphConvLayer(\n",
      "    (conv): HypergraphConv(8, 32)\n",
      "  )\n",
      "  (edge_conv): GraphConvolution()\n",
      "  (class_classifier): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      "  (norm1): CustomBatchNorm()\n",
      "  (norm2): CustomBatchNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_model = models.HypergraphModel(8, 3)\n",
    "print(train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from utils import accuracy, auc, precision, recall, f1_score\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "criteria = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 5e-4\n",
    "optimizer = optim.Adam(train_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "acc_measure = accuracy\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    train_model.train()\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    # print(processed_data[0].x[0:8].shape)\n",
    "    # print(processed_data[0].edge_weights.shape)\n",
    "    # print(processed_data[0].edge_index.shape)\n",
    "    for i in range(group_num):\n",
    "        print(i)\n",
    "        output = train_model(processed_data[i].x[:, 0:8], processed_data[i].edge_index, \n",
    "                                 processed_data[i].edge_weights, processed_data[i].x[:, 8:11], adj[i], T[i])\n",
    "        # print(output)\n",
    "        labels = processed_data[i].y\n",
    "        loss_train = criteria(output, labels)\n",
    "        # print(output[:, 1])\n",
    "        acc_train = acc_measure(output, labels)\n",
    "        \n",
    "        loss_train.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_model.eval()\n",
    "        output = train_model(processed_data[i].x[:, 0:8], processed_data[i].edge_index, \n",
    "                                 processed_data[i].edge_weights, processed_data[i].x[:, 8:11], adj[i], T[i])\n",
    "        # print(output)\n",
    "        loss_val = criteria(output, labels)\n",
    "        acc_val = acc_measure(output, labels)\n",
    "        auc_val = auc(output, labels)\n",
    "        per_val = precision(output, labels)\n",
    "        recall_val = recall(output, labels)\n",
    "        f1_val = f1_score(output, labels)\n",
    "        print(\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "            'auc_val: {:.4f}'.format(auc_val),\n",
    "            'per_val: {:.4f}'.format(per_val),\n",
    "            'recall_val: {:.4f}'.format(recall_val),\n",
    "            'f1_val: {:.4f}'.format(f1_val),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "    return loss_val.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    train_model.eval()\n",
    "    output = train_model(processed_data[group_num-1].x[:, 0:8], processed_data[group_num-1].edge_index, \n",
    "                             processed_data[group_num-1].edge_weights, processed_data[group_num-1].x[:, 8:11], adj[group_num-1], T[group_num-1])\n",
    "    labels = processed_data[group_num-1].y\n",
    "    \n",
    "    loss_test = criteria(output, labels)\n",
    "    acc_test = acc_measure(output, labels)\n",
    "    auc_test = auc(output, labels)\n",
    "    per_test = precision(output, labels)\n",
    "    recall_test = recall(output, labels)\n",
    "    f1_test = f1_score(output, labels)\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()),\n",
    "         'auc_test: {:.4f}'.format(auc_test),\n",
    "            'per_test: {:.4f}'.format(per_test),\n",
    "            'recall_test: {:.4f}'.format(recall_test),\n",
    "            'f1_test: {:.4f}'.format(f1_test))\n",
    "    return acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loss_train: 0.3794 acc_train: 0.9000 loss_val: 0.3675 acc_val: 0.9000 auc_val: 0.4889 per_val: 0.0000 recall_val: 0.0000 f1_val: 0.0000 time: 0.0211s\n",
      "1\n",
      "loss_train: 0.3368 acc_train: 0.9000 loss_val: 0.3354 acc_val: 0.9000 auc_val: 0.6533 per_val: 0.0000 recall_val: 0.0000 f1_val: 0.0000 time: 0.0781s\n",
      "2\n",
      "loss_train: 0.2689 acc_train: 0.9000 loss_val: 0.2689 acc_val: 0.9000 auc_val: 0.8178 per_val: 0.0000 recall_val: 0.0000 f1_val: 0.0000 time: 0.1138s\n",
      "3\n",
      "loss_train: 0.2937 acc_train: 0.9000 loss_val: 0.2930 acc_val: 0.9000 auc_val: 0.7600 per_val: 0.0000 recall_val: 0.0000 f1_val: 0.0000 time: 0.1464s\n",
      "4\n",
      "loss_train: 0.2782 acc_train: 0.9000 loss_val: 0.2782 acc_val: 0.9000 auc_val: 0.7867 per_val: 0.0000 recall_val: 0.0000 f1_val: 0.0000 time: 0.1780s\n",
      "5\n",
      "loss_train: 0.3049 acc_train: 0.9000 loss_val: 0.3038 acc_val: 0.9000 auc_val: 0.7778 per_val: 0.0000 recall_val: 0.0000 f1_val: 0.0000 time: 0.2154s\n",
      "6\n",
      "loss_train: 0.2700 acc_train: 0.9000 loss_val: 0.2700 acc_val: 0.9000 auc_val: 0.8267 per_val: 0.0000 recall_val: 0.0000 f1_val: 0.0000 time: 0.2532s\n",
      "7\n",
      "loss_train: 0.2533 acc_train: 0.9000 loss_val: 0.2534 acc_val: 0.9000 auc_val: 0.8000 per_val: 0.0000 recall_val: 0.0000 f1_val: 0.0000 time: 0.2849s\n",
      "8\n",
      "loss_train: 0.2631 acc_train: 0.9000 loss_val: 0.2635 acc_val: 0.9000 auc_val: 0.8489 per_val: 0.0000 recall_val: 0.0000 f1_val: 0.0000 time: 0.3158s\n",
      "9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m t_total \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m----> 7\u001B[0m     val \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 训练并获得结果\u001B[39;00m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m val \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# 检查是否返回了None值\u001B[39;00m\n\u001B[0;32m      9\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWarning: train(epoch) returned None for epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Skipping this iteration.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[9], line 30\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(epoch)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# print(output[:, 1])\u001B[39;00m\n\u001B[0;32m     28\u001B[0m acc_train \u001B[38;5;241m=\u001B[39m acc_measure(output, labels)\n\u001B[1;32m---> 30\u001B[0m \u001B[43mloss_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     33\u001B[0m train_model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\hypergraph\\lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\hypergraph\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\hypergraph\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "num_epochs = 1000\n",
    "early_stopping = 30\n",
    "val_watch = []\n",
    "t_total = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    val = train(epoch)  # 训练并获得结果\n",
    "    if val is None:  # 检查是否返回了None值\n",
    "        print(f\"Warning: train(epoch) returned None for epoch {epoch}. Skipping this iteration.\")\n",
    "        continue\n",
    "    \n",
    "    val_watch.append(val)  # 将结果添加到val_watch\n",
    "    test()\n",
    "    if epoch > early_stopping and val_watch[-1] > np.mean(val_watch[-(early_stopping + 1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "    \n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "print(\"Printing the weights : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}